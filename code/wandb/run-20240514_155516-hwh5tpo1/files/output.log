  0%|                                                                                                                                                                                                                                                                     | 0/100 [00:00<?, ?it/s]/home/icl2/anaconda3/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(



  5%|████████████▋                                                                                                                                                                                                                                                | 5/100 [01:03<20:11, 12.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (93 > 77). Running this sequence through the model will result in indexing errors
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['morbacher, female, feminine, art deco, new baroque, intricate linework']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['morbacher, female, feminine, art deco, new baroque, intricate linework']























 30%|███████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                | 30/100 [06:23<14:54, 12.79s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', tom bagshaw, lawrence alma - tadema, greg rutkowski, alphonse mucha']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', tom bagshaw, lawrence alma - tadema, greg rutkowski, alphonse mucha']























 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                 | 55/100 [11:44<09:43, 12.97s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['germ and greg rutkowski and alphonse mucha and loish and wlop']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['germ and greg rutkowski and alphonse mucha and loish and wlop']



 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                    | 60/100 [12:49<08:39, 13.00s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['new yorker magazine cover']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['new yorker magazine cover']

 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                             | 63/100 [13:28<08:00, 12.98s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [',']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [',']




 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                              | 69/100 [14:45<06:42, 12.98s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['rim light, unreal engine, 8 k, vibrant colors, smooth gradients, high contrast, depth of field']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['rim light, unreal engine, 8 k, vibrant colors, smooth gradients, high contrast, depth of field']




 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                               | 75/100 [16:03<05:24, 13.00s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['jeremy lipkin and giuseppe dangelico pino and michael garmash and rob rey']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['jeremy lipkin and giuseppe dangelico pino and michael garmash and rob rey']










 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 87/100 [18:39<02:48, 12.97s/it]The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and greg rutkowski and alphonse mucha and william - adolphe bouguereau, craig mullins, j. c. leyendecker �']
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and greg rutkowski and alphonse mucha and william - adolphe bouguereau, craig mullins, j. c. leyendecker �']
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 88/100 [19:04<02:36, 13.01s/it]
Traceback (most recent call last):
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 305, in <module>
    main()
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 216, in main
    original_image, recon_image, diff_image, original_latents_visualized, recon_latents_visualized, diff_latents_visualized, error_TOT, error_OTO, error_middle, output_loss, peak_gpu_allocated, inf_time, inv_time, seed, extra_outputs, extra_outputs_another, another_output = single_exp(seed, prompt, inversion_prompt,
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 48, in single_exp
    recon_images, recon_zo, recon_latents, output_loss, peak_gpu_allocated, dec_inv_time, extra_outputs, extra_outputs_another, another_output = insta_pipe.exact_inversion(
  File "/home/icl2/khlee/InstaFlow/code/pipeline_rf.py", line 955, in exact_inversion
    latents, extra_outputs, extra_outputs_another, another_output = self.dec_direct(image, latents, test_beta=test_beta, adam=False, decoder_inv_steps=decoder_inv_steps, decoder_lr=decoder_lr, verbose=verbose, use_float=decoder_use_float)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/icl2/khlee/InstaFlow/code/pipeline_rf.py", line 1489, in dec_direct
    cocoercivity_rate_array_another[i] = cocoercivity_rate
KeyboardInterrupt