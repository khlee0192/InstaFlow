
  0%|                                                                                                                                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s]/home/icl2/anaconda3/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
start, NMSE : 0.017723161727190018
1, NMSE : 0.01775277592241764, lr : 0.002
2, NMSE : 0.017842458561062813, lr : 0.004
3, NMSE : 0.01805245131254196, lr : 0.006
4, NMSE : 0.01847226731479168, lr : 0.008
5, NMSE : 0.019219323992729187, lr : 0.01
6, NMSE : 0.0204389039427042, lr : 0.012
7, NMSE : 0.022302227094769478, lr : 0.013999999999999999
8, NMSE : 0.02500266768038273, lr : 0.016
9, NMSE : 0.02875295840203762, lr : 0.018000000000000002
10, NMSE : 0.033784788101911545, lr : 0.02
11, NMSE : 0.039702773094177246, lr : 0.02
12, NMSE : 0.04647775739431381, lr : 0.019993908270190958
13, NMSE : 0.05407704785466194, lr : 0.019975640502598244
14, NMSE : 0.062465913593769073, lr : 0.019945218953682734
15, NMSE : 0.07160764187574387, lr : 0.019902680687415703
16, NMSE : 0.08146543800830841, lr : 0.01984807753012208
17, NMSE : 0.09200204163789749, lr : 0.019781476007338056
18, NMSE : 0.10318079590797424, lr : 0.019702957262759963
19, NMSE : 0.11496493220329285, lr : 0.01961261695938319
20, NMSE : 0.127317875623703, lr : 0.019510565162951538
21, NMSE : 0.14020363986492157, lr : 0.019396926207859086
22, NMSE : 0.15358702838420868, lr : 0.019271838545667875
23, NMSE : 0.16743314266204834, lr : 0.019135454576426007
24, NMSE : 0.18170711398124695, lr : 0.01898794046299167
25, NMSE : 0.19637423753738403, lr : 0.01882947592858927
26, NMSE : 0.21139976382255554, lr : 0.01866025403784439
27, NMSE : 0.22674967348575592, lr : 0.01848048096156426
28, NMSE : 0.24239090085029602, lr : 0.018290375725550416
29, NMSE : 0.25829142332077026, lr : 0.018090169943749474
30, NMSE : 0.2744203805923462, lr : 0.01788010753606722
31, NMSE : 0.29074791073799133, lr : 0.01766044443118978
32, NMSE : 0.3072442412376404, lr : 0.017431448254773944
33, NMSE : 0.3238804340362549, lr : 0.017193398003386512
34, NMSE : 0.3406284749507904, lr : 0.016946583704589974
35, NMSE : 0.35745975375175476, lr : 0.016691306063588582
36, NMSE : 0.37434613704681396, lr : 0.01642787609686539
37, NMSE : 0.39126044511795044, lr : 0.016156614753256582
38, NMSE : 0.40817612409591675, lr : 0.015877852522924733
39, NMSE : 0.42506781220436096, lr : 0.015591929034707469
40, NMSE : 0.441910058259964, lr : 0.01529919264233205
41, NMSE : 0.4586787819862366, lr : 0.015
42, NMSE : 0.47535088658332825, lr : 0.014694715627858908
43, NMSE : 0.4919035732746124, lr : 0.014383711467890775
44, NMSE : 0.5083144307136536, lr : 0.014067366430758002
45, NMSE : 0.5245615243911743, lr : 0.01374606593415912
46, NMSE : 0.5406241416931152, lr : 0.013420201433256689
47, NMSE : 0.5564827919006348, lr : 0.013090169943749475
48, NMSE : 0.5721189379692078, lr : 0.01275637355816999
49, NMSE : 0.5875146389007568, lr : 0.012419218955996679
50, NMSE : 0.6026525497436523, lr : 0.012079116908177595
51, NMSE : 0.6175155639648438, lr : 0.011736481776669305
52, NMSE : 0.6320872902870178, lr : 0.011391731009600656
53, NMSE : 0.646352231502533, lr : 0.011045284632676535
54, NMSE : 0.6602957844734192, lr : 0.010697564737441254
55, NMSE : 0.673904299736023, lr : 0.010348994967025012
56, NMSE : 0.6871645450592041, lr : 0.01
57, NMSE : 0.7000642418861389, lr : 0.009651005032974993
58, NMSE : 0.7125930786132812, lr : 0.009302435262558747
59, NMSE : 0.7247406840324402, lr : 0.008954715367323467
60, NMSE : 0.7364990711212158, lr : 0.008608268990399346
61, NMSE : 0.7478604912757874, lr : 0.008263518223330697
62, NMSE : 0.7588188052177429, lr : 0.00792088309182241
63, NMSE : 0.769368588924408, lr : 0.007580781044003322
64, NMSE : 0.7795055508613586, lr : 0.007243626441830009
65, NMSE : 0.7892264723777771, lr : 0.006909830056250526
66, NMSE : 0.7985293865203857, lr : 0.006579798566743313
67, NMSE : 0.8074131608009338, lr : 0.00625393406584088
68, NMSE : 0.8158778548240662, lr : 0.0059326335692419995
69, NMSE : 0.8239244222640991, lr : 0.005616288532109225
70, NMSE : 0.8315550684928894, lr : 0.005305284372141095
71, NMSE : 0.8387730717658997, lr : 0.005000000000000003
72, NMSE : 0.8455823063850403, lr : 0.004700807357667952
73, NMSE : 0.8519881367683411, lr : 0.0044080709652925334
74, NMSE : 0.8579962253570557, lr : 0.0041221474770752695
75, NMSE : 0.8636134266853333, lr : 0.0038433852467434173
76, NMSE : 0.8688478469848633, lr : 0.0035721239031346066
77, NMSE : 0.8737074732780457, lr : 0.0033086939364114177
78, NMSE : 0.8782018423080444, lr : 0.0030534162954100295
79, NMSE : 0.8823413848876953, lr : 0.002806601996613488
80, NMSE : 0.8861365914344788, lr : 0.0025685517452260598
81, NMSE : 0.8895992040634155, lr : 0.002339555568810221
82, NMSE : 0.892741322517395, lr : 0.002119892463932781
83, NMSE : 0.8955761194229126, lr : 0.0019098300562505265
84, NMSE : 0.8981168866157532, lr : 0.0017096242744495838
85, NMSE : 0.9003773927688599, lr : 0.0015195190384357405
86, NMSE : 0.902372419834137, lr : 0.001339745962155613
87, NMSE : 0.904117226600647, lr : 0.0011705240714107324
88, NMSE : 0.9056268930435181, lr : 0.0010120595370083296
89, NMSE : 0.9069173336029053, lr : 0.0008645454235739925
90, NMSE : 0.9080049991607666, lr : 0.0007281614543321269
91, NMSE : 0.9089062809944153, lr : 0.0006030737921409169
92, NMSE : 0.909637987613678, lr : 0.0004894348370484647
93, NMSE : 0.9102174639701843, lr : 0.0003873830406168133
94, NMSE : 0.910662055015564, lr : 0.00029704273724003526
95, NMSE : 0.9109892249107361, lr : 0.0002185239926619431
96, NMSE : 0.9112164974212646, lr : 0.0001519224698779198
97, NMSE : 0.9113624691963196, lr : 9.73193125842975e-05
98, NMSE : 0.9114445447921753, lr : 5.47810463172671e-05
99, NMSE : 0.9114810228347778, lr : 2.4359497401758025e-05
100, NMSE : 0.9114900827407837, lr : 6.091729809042379e-06
  0%|                                                                                                                                                                                                                                                                                                        | 0/1 [00:22<?, ?it/s]
Traceback (most recent call last):
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 241, in <module>
    main()
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 180, in main
    original_image, recon_image, diff_image, original_latents_visualized, recon_latents_visualized, diff_latents_visualized, error_TOT, error_OTO, error_middle, output_loss, peak_gpu_allocated, inf_time, inv_time, seed = single_exp(seed, prompt, inversion_prompt,
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/icl2/khlee/InstaFlow/code/main_new_algorithm.py", line 47, in single_exp
    recon_images, recon_zo, recon_latents, output_loss, peak_gpu_allocated, dec_inv_time = insta_pipe.exact_inversion(
  File "/home/icl2/khlee/InstaFlow/code/pipeline_rf.py", line 997, in exact_inversion
    v_pred = self.unet(latent_model_input, vec_t, encoder_hidden_states=prompt_embeds).sample
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py", line 932, in forward
    emb = self.time_embedding(t_emb, timestep_cond)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/diffusers/models/embeddings.py", line 226, in forward
    sample = self.linear_1(sample)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/diffusers/models/lora.py", line 430, in forward
    out = super().forward(hidden_states)
  File "/home/icl2/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half